{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from pandas) (2022.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
            "Requirement already satisfied: transformers in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (4.16.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: requests in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: sacremoses in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from transformers) (0.0.53)\n",
            "Requirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from transformers) (2023.5.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests->transformers) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: click in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from sacremoses->transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from sacremoses->transformers) (1.2.0)\n",
            "Requirement already satisfied: six in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from sacremoses->transformers) (1.16.0)\n",
            "Requirement already satisfied: fsspec in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (2023.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.6.0)\n",
            "Requirement already satisfied: torch in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (1.12.0)\n",
            "Requirement already satisfied: typing-extensions in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from torch) (4.6.0)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (7.352.0)\n",
            "Requirement already satisfied: cloudpickle==2.2.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (2.2.1)\n",
            "Requirement already satisfied: mlflow==2.4 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (2.4.0)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mlflow==2.4) (2.0.19)\n",
            "Requirement already satisfied: gunicorn<21; platform_system != \"Windows\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mlflow==2.4) (20.1.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mlflow==2.4) (8.1.3)\n",
            "Requirement already satisfied: entrypoints<1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mlflow==2.4) (0.4)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mlflow==2.4) (2.31.0)\n",
            "Requirement already satisfied: matplotlib<4 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mlflow==2.4) (3.2.1)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mlflow==2.4) (0.4.4)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mlflow==2.4) (6.0)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mlflow==2.4) (1.11.1)\n",
            "Requirement already satisfied: cloudpickle<3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mlflow==2.4) (2.2.1)\n",
            "Requirement already satisfied: gitpython<4,>=2.1.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mlflow==2.4) (3.1.31)\n",
            "Requirement already satisfied: protobuf<5,>=3.12.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mlflow==2.4) (3.20.3)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11; platform_system != \"Windows\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mlflow==2.4) (2.11.2)\n",
            "Requirement already satisfied: packaging<24 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mlflow==2.4) (23.0)\n",
            "Requirement already satisfied: numpy<2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mlflow==2.4) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mlflow==2.4) (6.6.0)\n",
            "Requirement already satisfied: databricks-cli<1,>=0.8.7 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mlflow==2.4) (0.17.7)\n",
            "Requirement already satisfied: scipy<2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mlflow==2.4) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn<2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mlflow==2.4) (0.22.1)\n",
            "Requirement already satisfied: docker<7,>=4.0.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mlflow==2.4) (6.1.2)\n",
            "Requirement already satisfied: pytz<2024 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mlflow==2.4) (2022.5)\n",
            "Requirement already satisfied: Flask<3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mlflow==2.4) (2.3.2)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mlflow==2.4) (3.4.3)\n",
            "Requirement already satisfied: pyarrow<13,>=4.0.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mlflow==2.4) (9.0.0)\n",
            "Requirement already satisfied: querystring-parser<2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mlflow==2.4) (1.2.4)\n",
            "Requirement already satisfied: pandas<3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from mlflow==2.4) (1.1.5)\n",
            "Requirement already satisfied: greenlet!=0.4.17; platform_machine == \"aarch64\" or (platform_machine == \"ppc64le\" or (platform_machine == \"x86_64\" or (platform_machine == \"amd64\" or (platform_machine == \"AMD64\" or (platform_machine == \"win32\" or platform_machine == \"WIN32\"))))) in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from sqlalchemy<3,>=1.4.0->mlflow==2.4) (2.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from sqlalchemy<3,>=1.4.0->mlflow==2.4) (4.6.0)\n",
            "Requirement already satisfied: setuptools>=3.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from gunicorn<21; platform_system != \"Windows\"->mlflow==2.4) (65.6.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow==2.4) (2022.9.24)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow==2.4) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow==2.4) (3.1.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow==2.4) (1.26.16)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from matplotlib<4->mlflow==2.4) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from matplotlib<4->mlflow==2.4) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from matplotlib<4->mlflow==2.4) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from matplotlib<4->mlflow==2.4) (1.4.4)\n",
            "Requirement already satisfied: Mako in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from alembic!=1.10.0,<2->mlflow==2.4) (1.2.4)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from alembic!=1.10.0,<2->mlflow==2.4) (5.12.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from gitpython<4,>=2.1.0->mlflow==2.4) (4.0.10)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from Jinja2<4,>=2.11; platform_system != \"Windows\"->mlflow==2.4) (2.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow==2.4) (3.12.0)\n",
            "Requirement already satisfied: pyjwt>=1.7.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from databricks-cli<1,>=0.8.7->mlflow==2.4) (2.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from databricks-cli<1,>=0.8.7->mlflow==2.4) (1.16.0)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from databricks-cli<1,>=0.8.7->mlflow==2.4) (0.9.0)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from databricks-cli<1,>=0.8.7->mlflow==2.4) (3.2.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from scikit-learn<2->mlflow==2.4) (1.2.0)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from docker<7,>=4.0.0->mlflow==2.4) (1.3.3)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from Flask<3->mlflow==2.4) (2.1.2)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from Flask<3->mlflow==2.4) (1.6.2)\n",
            "Requirement already satisfied: Werkzeug>=2.3.3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from Flask<3->mlflow==2.4) (2.3.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow==2.4) (5.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install nvidia-ml-py3\n",
        "!pip install --no-cache-dir cloudpickle==2.2.1\n",
        "!pip install mlflow==2.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1689887328392
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch \n",
        "import numpy as np\n",
        "from transformers import BertTokenizerFast, BertForTokenClassification\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from torch.optim import SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1689647143509
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023/07/18 02:25:42 WARNING mlflow.pyfunc: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
            " - numpy (current: 1.21.6, required: numpy==1.21.2)\n",
            " - packaging (current: 23.0, required: packaging==23.1)\n",
            " - torch (current: 1.12.0, required: torch==2.0.1)\n",
            " - transformers (current: 4.16.0, required: transformers==4.30.0)\n",
            "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Model will use CPU runtime\n"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "model = mlflow.pyfunc.load_model('models:/uc1_token/latest')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1689647308494
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['B-BRAND',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'B-PATTERN',\n",
              " 'I-PATTERN',\n",
              " 'I-PATTERN',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample =  pd.DataFrame({ 'text':[\"Gaok Men's Retro Cotton Multi-Pocket Camo Cargo Shorts\"] })\n",
        "model.predict(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1689887332552
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "df = pd.read_csv('./data/output_fashion.csv', names=[\"text\", \"labels\", \"category\"], skiprows=1)\n",
        "\n",
        "# Drop the last column\n",
        "df = df.drop(columns=\"category\")\n",
        "df = df.head(100)\n",
        "# Function to convert the string representation to a list\n",
        "def convert_to_list(string_representation):\n",
        "    return ast.literal_eval(string_representation)\n",
        "\n",
        "# Apply the function to the 'bio_tag' column for all rows\n",
        "df['labels'] = df['labels'].apply(convert_to_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1689887332755
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "labels = [i for i in df['labels'].values.tolist()]\n",
        "unique_labels = set()\n",
        "\n",
        "for lb in labels:\n",
        "    [unique_labels.add(i) for i in lb if i not in unique_labels]\n",
        "\n",
        "\n",
        "# Sort the unique_labels\n",
        "sorted_unique_labels = sorted(unique_labels)\n",
        "\n",
        "# Create labels_to_ids and ids_to_labels mappings\n",
        "labels_to_ids = {label: idx for idx, label in enumerate(sorted_unique_labels)}\n",
        "ids_to_labels = {idx: label for idx, label in enumerate(sorted_unique_labels)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1689887332935
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Function to convert the space-separated string to a list\n",
        "def convert_to_list(string_representation):\n",
        "    return \" \".join(string_representation)\n",
        "\n",
        "# Apply the function to the 'bio_tag' column for all rows\n",
        "df['labels'] = df['labels'].apply(convert_to_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1689793000120
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def extract_entities_from_bio(sentence, bio_tags):\n",
        "    entities = {}\n",
        "    current_entity = None\n",
        "    current_entity_type = None\n",
        "    words = sentence.split()\n",
        "    tags = bio_tags.split()\n",
        "\n",
        "    for word, tag in zip(words, tags):\n",
        "        if tag.startswith(\"B-\"):\n",
        "            if current_entity:\n",
        "                entity_value = \" \".join(current_entity)\n",
        "                entities[entity_value] = current_entity_type\n",
        "\n",
        "            current_entity = [word]\n",
        "            current_entity_type = tag[2:]\n",
        "        elif tag.startswith(\"I-\"):\n",
        "            if current_entity:\n",
        "                current_entity.append(word)\n",
        "        else:\n",
        "            if current_entity:\n",
        "                entity_value = \" \".join(current_entity)\n",
        "                entities[entity_value] = current_entity_type\n",
        "                current_entity = None\n",
        "                current_entity_type = None\n",
        "\n",
        "    if current_entity:\n",
        "        entity_value = \" \".join(current_entity)\n",
        "        entities[entity_value] = current_entity_type\n",
        "\n",
        "    return entities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1689793000293
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 68\u001b[0m\n\u001b[1;32m     66\u001b[0m category_output \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m     67\u001b[0m sample \u001b[38;5;241m=\u001b[39m  pd\u001b[38;5;241m.\u001b[39mDataFrame({ \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m:[item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m]]})\n\u001b[0;32m---> 68\u001b[0m bio_tags \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(sample))\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Extract entities for the \"Title\" attribute using the given function\u001b[39;00m\n\u001b[1;32m     70\u001b[0m title_entities \u001b[38;5;241m=\u001b[39m extract_entities_from_bio(item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m\"\u001b[39m], bio_tags)  \u001b[38;5;66;03m# Assuming 'bio_tags' is defined earlier\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from collections import OrderedDict\n",
        "\n",
        "# The extract_entities_from_bio function (keep it as it is)\n",
        "\n",
        "# Given JSON input\n",
        "json_input = '''\n",
        "{\n",
        "\"data\": [\n",
        "\t\t{\n",
        "\t\t\t\"Title\": \"Gaok Men's Retro Cotton Multi-Pocket Camo Cargo Shorts\",\n",
        "\t\t\t\"Description\": \"Random Description\",\n",
        "\t\t\t\"Price\": \"13\",\n",
        "\t\t\t\"Category\": \"Jeany Jeans\",\n",
        "\t\t\t\"Image\": \"https://www.ssxsds.de/test.jpg\"\n",
        "\t\t},\n",
        "\t\t{\n",
        "\t\t\t\"Title\": \"Honchosfx Mens Retro Creedence Clearwater Revival 1971 T-Shirt Brown\",\n",
        "\t\t\t\"Description\": \"ssdsdsdssdssd\",\n",
        "\t\t\t\"Price\": \"15\",\n",
        "\t\t\t\"Category\": \"random\",\n",
        "\t\t\t\"Image\": \"https://www.ssxsds.de/test.jpg\"\n",
        "\t\t}\n",
        "\t]\n",
        "}\n",
        "'''\n",
        "\n",
        "# Load the JSON input\n",
        "data = json.loads(json_input)\n",
        "\n",
        "# Initialize the output_data with the given structure\n",
        "output_data = {\n",
        "    \"status\": \"success\",\n",
        "    \"result\": [{\n",
        "        \"general\": [{\n",
        "            \"types\": [\"Dog supplies\"],\n",
        "            \"all_attributes\": [{\n",
        "                \"Title\": \"string\",\n",
        "                \"Description\": \"string\",\n",
        "                \"Price\": \"decimal\",\n",
        "                \"Category\": \"string\",\n",
        "                \"Type\": \"string\",\n",
        "                \"Image\": \"file\"\n",
        "            }],\n",
        "            \"common_attributes\": [{\n",
        "                \"Title\": \"string\",\n",
        "                \"Description\": \"string\",\n",
        "                \"Price\": \"file\",\n",
        "                \"Category\": \"string\"\n",
        "            }]\n",
        "        }]\n",
        "    }],\n",
        "    \"data\": []\n",
        "}\n",
        "\n",
        "# Create the \"tags\" field for the \"Category\" key\n",
        "tags_data = [{\n",
        "    \"attribute\": \"category\",\n",
        "    \"probability\": \"100%\"\n",
        "}]\n",
        "\n",
        "# Loop through each data item and process them individually\n",
        "for item in data[\"data\"]:\n",
        "    # Create an ordered dictionary to maintain the key order for each data item\n",
        "    category_output = OrderedDict()\n",
        "    sample =  pd.DataFrame({ 'text':[item['Title']]})\n",
        "    bio_tags = \" \".join(model.predict(sample))\n",
        "    # Extract entities for the \"Title\" attribute using the given function\n",
        "    title_entities = extract_entities_from_bio(item[\"Title\"], bio_tags)  # Assuming 'bio_tags' is defined earlier\n",
        "\n",
        "    # Add the keys in the desired order for the current data item\n",
        "    for key in [\"Title\", \"Description\", \"Price\", \"Category\", \"Image\"]:\n",
        "        if key == \"Category\":\n",
        "            category_output[key] = [\n",
        "                {\n",
        "                    \"input\": item[key],\n",
        "                    \"status\": \"mapped\",\n",
        "                    \"tags\": {\n",
        "                        item[key]: tags_data\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        elif key == \"Title\":\n",
        "            if title_entities:\n",
        "                title_tags = []\n",
        "                for entity_key, entity_value in title_entities.items():\n",
        "                    if entity_key in item[key]:\n",
        "                        title_tags.append({entity_key: [{\"attribute\": entity_value}]})\n",
        "                category_output[key] = [\n",
        "                    {\n",
        "                        \"input\": item[key],\n",
        "                        \"status\": \"mapped\",\n",
        "                        \"tags\": title_tags\n",
        "                    }\n",
        "                ]\n",
        "            else:\n",
        "                category_output[key] = [\n",
        "                    {\n",
        "                        \"input\": item[key],\n",
        "                        \"status\": \"unmapped\"\n",
        "                    }\n",
        "                ]\n",
        "        else:\n",
        "            category_output[key] = [\n",
        "                {\n",
        "                    \"input\": item[key],\n",
        "                    \"status\": \"unmapped\"\n",
        "                }\n",
        "            ]\n",
        "\n",
        "    # Add the output format for the current data item to the \"data\" list\n",
        "    output_data[\"data\"].append(category_output)\n",
        "\n",
        "# Save the output data to a new JSON file\n",
        "with open(\"output1.json\", \"w\") as f:\n",
        "    json.dump(output_data, f, indent=4)\n",
        "\n",
        "print(\"Successfully created 'output3.json' with the desired output format.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1689887359172
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
        "\n",
        "import torch\n",
        "label_all_tokens = False\n",
        "\n",
        "def align_label(texts, labels):\n",
        "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=120, truncation=True)\n",
        "\n",
        "    word_ids = tokenized_inputs.word_ids()\n",
        "\n",
        "    previous_word_idx = None\n",
        "    label_ids = []\n",
        "\n",
        "    for word_idx in word_ids:\n",
        "\n",
        "        if word_idx is None:\n",
        "            label_ids.append(-100)\n",
        "\n",
        "        elif word_idx != previous_word_idx:\n",
        "            try:\n",
        "                label_ids.append(labels_to_ids[labels[word_idx]])\n",
        "            except:\n",
        "                label_ids.append(-100)\n",
        "        else:\n",
        "            try:\n",
        "                label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n",
        "            except:\n",
        "                label_ids.append(-100)\n",
        "        previous_word_idx = word_idx\n",
        "\n",
        "    return label_ids\n",
        "\n",
        "class DataSequence(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "\n",
        "        lb = [i.split() for i in df['labels'].values.tolist()]\n",
        "        txt = df['text'].values.tolist()\n",
        "        self.texts = [tokenizer(str(i),\n",
        "                               padding='max_length', max_length = 120, truncation=True, return_tensors=\"pt\") for i in txt]\n",
        "        self.labels = [align_label(i,j) for i,j in zip(txt, lb)]\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_data(self, idx):\n",
        "\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "\n",
        "        return torch.LongTensor(self.labels[idx])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_data = self.get_batch_data(idx)\n",
        "        batch_labels = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1689887364546
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Shuffle the DataFrame rows\n",
        "df = df.sample(frac=1, random_state=42)\n",
        "\n",
        "# Calculate the indices to split the DataFrame\n",
        "train_size = int(0.9 * len(df))\n",
        "val_size = len(df) - train_size\n",
        "\n",
        "# Split the DataFrame into df_train and df_val\n",
        "df_train = df[:train_size]\n",
        "df_val = df[train_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1689887367886
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "class BertModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(BertModel, self).__init__()\n",
        "\n",
        "        self.bert = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(sorted_unique_labels))\n",
        "\n",
        "    def forward(self, input_id, mask, label):\n",
        "\n",
        "        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1689887482877
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6547df955cc4bf692e2bb4ac883dfe0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|██████████| 23/23 [00:00<00:00, 536.03it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 3070.50it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 3902.47it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 3681.60it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 5251.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23\n",
            "23\n",
            "23\n",
            "23\n",
            "23\n"
          ]
        }
      ],
      "source": [
        "def train_loop(model, df_train, df_val):\n",
        "\n",
        "    train_dataset = DataSequence(df_train)\n",
        "    val_dataset = DataSequence(df_val)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    optimizer = SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    if use_cuda:\n",
        "        model = model.cuda()\n",
        "\n",
        "    best_acc = 0\n",
        "    best_loss = 1000\n",
        "\n",
        "    for epoch_num in range(EPOCHS):\n",
        "\n",
        "        total_acc_train = 0\n",
        "        total_loss_train = 0\n",
        "        count = 0\n",
        "\n",
        "        #model.train()\n",
        "\n",
        "        for train_data, train_label in tqdm(train_dataloader):\n",
        "            count = count + 1\n",
        "\n",
        "            train_label = train_label.to(device)\n",
        "            mask = train_data['attention_mask'].squeeze(1).to(device)\n",
        "            input_id = train_data['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss, logits = model(input_id, mask, train_label)\n",
        "\n",
        "            for i in range(logits.shape[0]):\n",
        "\n",
        "              logits_clean = logits[i][train_label[i] != -100]\n",
        "              label_clean = train_label[i][train_label[i] != -100]\n",
        "\n",
        "              predictions = logits_clean.argmax(dim=1)\n",
        "              acc = (predictions == label_clean).float().mean()\n",
        "              total_acc_train += acc\n",
        "              total_loss_train += loss.item()\n",
        "              \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        total_acc_val = 0\n",
        "        total_loss_val = 0\n",
        "\n",
        "        for val_data, val_label in val_dataloader:\n",
        "\n",
        "            val_label = val_label.to(device)\n",
        "            mask = val_data['attention_mask'].squeeze(1).to(device)\n",
        "            input_id = val_data['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "            loss, logits = model(input_id, mask, val_label)\n",
        "\n",
        "            for i in range(logits.shape[0]):\n",
        "\n",
        "              logits_clean = logits[i][val_label[i] != -100]\n",
        "              label_clean = val_label[i][val_label[i] != -100]\n",
        "\n",
        "              predictions = logits_clean.argmax(dim=1)\n",
        "              acc = (predictions == label_clean).float().mean()\n",
        "              total_acc_val += acc\n",
        "              total_loss_val += loss.item()\n",
        "\n",
        "        print(\n",
        "            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(df_train): .3f} | Accuracy: {total_acc_train: .3f} | Val_Loss: {total_loss_val / len(df_val): .3f} | Accuracy: {total_acc_val / len(df_val): .3f}')\n",
        "LEARNING_RATE = 5e-3\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 4\n",
        "model = BertModel()\n",
        "train_loop(model, df_train, df_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1689475625797
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./outputs/model21.pth/tokenizer_config.json',\n",
              " './outputs/model21.pth/special_tokens_map.json',\n",
              " './outputs/model21.pth/vocab.txt',\n",
              " './outputs/model21.pth/added_tokens.json',\n",
              " './outputs/model21.pth/tokenizer.json')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save the trained model\n",
        "model_path = './outputs/model21.pth'\n",
        "\n",
        "\n",
        "# Save the model using save_pretrained\n",
        "model.bert.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1689890158942
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'BertForTokenClassification' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./outputs/model21.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Load the model using from_pretrained\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model1 \u001b[38;5;241m=\u001b[39m \u001b[43mBertForTokenClassification\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#model = BertForSequenceClassification.from_pretrained(model_path, num_labels=len(target_names))\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizerFast\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BertForTokenClassification' is not defined"
          ]
        }
      ],
      "source": [
        "model_path = './outputs/model21.pth'\n",
        "# Load the model using from_pretrained\n",
        "model1 = BertForTokenClassification.from_pretrained(model_path).to(\"cuda\")\n",
        "#model = BertForSequenceClassification.from_pretrained(model_path, num_labels=len(target_names))\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1689942499675
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "\n",
        "from azure.ai.ml import MLClient, Input\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "from azure.ai.ml import load_component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1689942501654
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Handle to the workspace\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "# Authentication package\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "\n",
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    # Check if given credential can get token successfully.\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "    credential = InteractiveBrowserCredential()\n",
        "\n",
        "# Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=\"\",\n",
        "    resource_group_name=\"\",\n",
        "    workspace_name=\"\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1689843400132
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'{\"data\": [{\"Title\": \"Gaok Men\\'s Retro Cotton Multi-Pocket Camo Cargo Shorts\", \"Description\": \"Random Description\", \"Price\": \"13\", \"Category\": \"Jeany Jeans\", \"Image\": \"https://www.ssxsds.de/test.jpg\"}, {\"Title\": \"Honchosfx Mens Retro Creedence Clearwater Revival 1971 T-Shirt Brown\", \"Description\": \"ssdsdsdssdssd\", \"Price\": \"15\", \"Category\": \"random\", \"Image\": \"https://www.ssxsds.de/test.jpg\"}]}'\n",
            "b'{\"status\": \"success\", \"result\": [{\"general\": [{\"types\": [\"Dog supplies\"], \"all_attributes\": [{\"Title\": \"string\", \"Description\": \"string\", \"Price\": \"decimal\", \"Category\": \"string\", \"Type\": \"string\", \"Image\": \"file\"}], \"common_attributes\": [{\"Title\": \"string\", \"Description\": \"string\", \"Price\": \"file\", \"Category\": \"string\"}]}]}], \"data\": [{\"Title\": [{\"input\": \"Gaok Men\\'s Retro Cotton Multi-Pocket Camo Cargo Shorts\", \"status\": \"mapped\", \"tags\": [{\"Gaok\": [{\"attribute\": \"BRAND\"}]}, {\"Camo Cargo Shorts\": [{\"attribute\": \"PATTERN\"}]}]}], \"Description\": [{\"input\": \"Random Description\", \"status\": \"unmapped\"}], \"Price\": [{\"input\": \"13\", \"status\": \"unmapped\"}], \"Category\": [{\"input\": \"Jeany Jeans\", \"status\": \"mapped\", \"tags\": {\"Jeany Jeans\": [{\"attribute\": \"category\", \"probability\": \"100%\"}]}}], \"Image\": [{\"input\": \"https://www.ssxsds.de/test.jpg\", \"status\": \"unmapped\"}]}, {\"Title\": [{\"input\": \"Honchosfx Mens Retro Creedence Clearwater Revival 1971 T-Shirt Brown\", \"status\": \"mapped\", \"tags\": [{\"Honchosfx\": [{\"attribute\": \"BRAND\"}]}]}], \"Description\": [{\"input\": \"ssdsdsdssdssd\", \"status\": \"unmapped\"}], \"Price\": [{\"input\": \"15\", \"status\": \"unmapped\"}], \"Category\": [{\"input\": \"random\", \"status\": \"mapped\", \"tags\": {\"random\": [{\"attribute\": \"category\", \"probability\": \"100%\"}]}}], \"Image\": [{\"input\": \"https://www.ssxsds.de/test.jpg\", \"status\": \"unmapped\"}]}]}'\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import json\n",
        "import os\n",
        "import ssl\n",
        "\n",
        "def allowSelfSignedHttps(allowed):\n",
        "    # bypass the server certificate verification on client side\n",
        "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n",
        "\n",
        "# Request data goes here\n",
        "# The example below assumes JSON formatting which may be updated\n",
        "# depending on the format your endpoint expects.\n",
        "# More information can be found here:\n",
        "# https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script\n",
        "data = {\n",
        "\t\"data\": [\n",
        "\t\t{\n",
        "\t\t\t\"Title\": \"Gaok Men's Retro Cotton Multi-Pocket Camo Cargo Shorts\",\n",
        "\t\t\t\"Description\": \"Random Description\",\n",
        "\t\t\t\"Price\": \"13\",\n",
        "\t\t\t\"Category\": \"Jeany Jeans\",\n",
        "\t\t\t\"Image\": \"https://www.ssxsds.de/test.jpg\"\n",
        "\t\t},\n",
        "\t\t{\n",
        "\t\t\t\"Title\": \"Honchosfx Mens Retro Creedence Clearwater Revival 1971 T-Shirt Brown\",\n",
        "\t\t\t\"Description\": \"ssdsdsdssdssd\",\n",
        "\t\t\t\"Price\": \"15\",\n",
        "\t\t\t\"Category\": \"random\",\n",
        "\t\t\t\"Image\": \"https://www.ssxsds.de/test.jpg\"\n",
        "\t\t}\n",
        "\t]\n",
        "}\t\n",
        "\n",
        "body = str.encode(json.dumps(data))\n",
        "\n",
        "url = ''\n",
        "# Replace this with the primary/secondary key or AMLToken for the endpoint\n",
        "api_key = ''\n",
        "if not api_key:\n",
        "    raise Exception(\"A key should be provided to invoke the endpoint\")\n",
        "\n",
        "# The azureml-model-deployment header will force the request to go to a specific deployment.\n",
        "# Remove this header to have the request observe the endpoint traffic rules\n",
        "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key), 'azureml-model-deployment': 'uc1' }\n",
        "\n",
        "req = urllib.request.Request(url, body, headers)\n",
        "try:\n",
        "    response = urllib.request.urlopen(req)\n",
        "\n",
        "    result = response.read()\n",
        "    print(result)\n",
        "except urllib.error.HTTPError as error:\n",
        "    print(\"The request failed with status code: \" + str(error.code))\n",
        "\n",
        "    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
        "    print(error.info())\n",
        "    print(error.read().decode(\"utf8\", 'ignore'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1689942506313
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment with name ml-use-case-1 is registered to workspace, the environment version is 19\n"
          ]
        }
      ],
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "custom_env_name = \"ml-use-case-1\"\n",
        "\n",
        "pipeline_job_env = Environment(\n",
        "    name=custom_env_name,\n",
        "    version=\"19\",\n",
        ")\n",
        "\n",
        "\n",
        "print(\n",
        "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1689944692294
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml import Input\n",
        "from azure.ai.ml import Output\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "# === Note on path ===\n",
        "# can be can be a local path or a cloud path. AzureML supports https://`, `abfss://`, `wasbs://` and `azureml://` URIs.\n",
        "# Local paths are automatically uploaded to the default datastore in the cloud.\n",
        "# More details on supported paths: https://docs.microsoft.com/azure/machine-learning/how-to-read-write-data-v2#supported-paths\n",
        "\n",
        "inputs = {\n",
        "    \"data_dir\": Input(\n",
        "        type=AssetTypes.URI_FILE, path=''\n",
        "    ),  # path=\"azureml:azureml_stoic_cartoon_wgb3lgvgky_output_data_cifar:1\"), #path=\"azureml://datastores/workspaceblobstore/paths/azureml/stoic_cartoon_wgb3lgvgky/cifar/\"),\n",
        "    \"epochs\":50,\n",
        "    \"batch_size\": 16,\n",
        "    \"learning_rate\": 2e-3 ,\n",
        "}\n",
        "\n",
        "job = command(\n",
        "    code=\"./\",  # local path where the code is stored\n",
        "    command=\"python demo.py --data_dir ${{inputs.data_dir}} --epochs ${{inputs.epochs}} --batch_size ${{inputs.batch_size}} --learning_rate ${{inputs.learning_rate}}\",\n",
        "    inputs=inputs,\n",
        "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
        "    compute=\"gpu\",  # Change the name to the gpu cluster of your workspace.\n",
        "    instance_count=1,  # In this, only 2 node cluster was created.\n",
        "    distribution={\n",
        "        \"type\": \"PyTorch\",\n",
        "        # set process count to the number of gpus per node\n",
        "        # NV6 has only 1 GPU\n",
        "        \"process_count_per_instance\": 4,\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1689944718419
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Your file exceeds 100 MB. If you experience low speeds, latency, or broken connections, we recommend using the AzCopyv10 tool for this file transfer.\n",
            "\n",
            "Example: azcopy copy '/mnt/batch/tasks/shared/LS_root/mounts/clusters/safira-cpu/code/Users/r.mandal/UC1_bio' 'https://safiraml9781015086.blob.core.windows.net/584b2160-4-8b11e43e-0492-5dd2-aef8-5b439a05f82c/UC1_bio' \n",
            "\n",
            "See https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-v10 for more information.\n",
            "\u001b[32mUploading UC1_bio (141.22 MBs): 100%|██████████| 141223221/141223221 [00:01<00:00, 119749718.43it/s]\n",
            "\u001b[39m\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>UC1_bio</td><td>olive_ticket_d3jylgfzrb</td><td>command</td><td>Starting</td><td><a href=\"https://ml.azure.com/runs/olive_ticket_d3jylgfzrb?wsid=/subscriptions/a4cdf085-0f39-444f-9cdc-ec6e91dfedf3/resourcegroups/a.anding-rg/workspaces/safira-ml&amp;tid=3c243ae2-542b-4e73-9e4f-525c99011556\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
            ],
            "text/plain": [
              "Command({'parameters': {}, 'init': False, 'name': 'olive_ticket_d3jylgfzrb', 'type': 'command', 'status': 'Starting', 'log_files': None, 'description': None, 'tags': {}, 'properties': {'_azureml.ComputeTargetType': 'amlctrain', 'ContentSnapshotId': 'd4c57b05-a95c-4013-a777-99297ade8d3f'}, 'print_as_yaml': True, 'id': '/subscriptions/a4cdf085-0f39-444f-9cdc-ec6e91dfedf3/resourceGroups/a.anding-rg/providers/Microsoft.MachineLearningServices/workspaces/safira-ml/jobs/olive_ticket_d3jylgfzrb', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/safira-cpu/code/Users/r.mandal/UC1_bio', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f82741fe770>, 'serialize': <msrest.serialization.Serializer object at 0x7f82741fd120>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': 'olive_ticket_d3jylgfzrb', 'experiment_name': 'UC1_bio', 'compute': 'gpu', 'services': {'Tracking': {'endpoint': 'azureml://eastus.api.azureml.ms/mlflow/v1.0/subscriptions/a4cdf085-0f39-444f-9cdc-ec6e91dfedf3/resourceGroups/a.anding-rg/providers/Microsoft.MachineLearningServices/workspaces/safira-ml?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/olive_ticket_d3jylgfzrb?wsid=/subscriptions/a4cdf085-0f39-444f-9cdc-ec6e91dfedf3/resourcegroups/a.anding-rg/workspaces/safira-ml&tid=3c243ae2-542b-4e73-9e4f-525c99011556', 'type': 'Studio'}}, 'comment': None, 'job_inputs': {'data_dir': {'type': 'uri_file', 'path': 'azureml://subscriptions/a4cdf085-0f39-444f-9cdc-ec6e91dfedf3/resourcegroups/a.anding-rg/workspaces/safira-ml/datastores/workspaceblobstore/paths/UI/2023-07-19_013839_UTC/output_fashion.csv', 'mode': 'ro_mount'}, 'epochs': '50', 'batch_size': '16', 'learning_rate': '0.002'}, 'job_outputs': {'default': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.olive_ticket_d3jylgfzrb', 'mode': 'rw_mount'}}, 'inputs': {'data_dir': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f82741ffe80>, 'epochs': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f82741fc580>, 'batch_size': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f82741ff2b0>, 'learning_rate': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f82741ff280>}, 'outputs': {'default': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f82741ffc70>}, 'component': CommandComponent({'intellectual_property': None, 'auto_increment_version': True, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': False, 'auto_delete_setting': None, 'name': 'olive_ticket_d3jylgfzrb', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': PosixPath('.'), 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f82741fe770>, 'serialize': <msrest.serialization.Serializer object at 0x7f82741ffdf0>, 'command': 'python demo.py --data_dir ${{inputs.data_dir}} --epochs ${{inputs.epochs}} --batch_size ${{inputs.batch_size}} --learning_rate ${{inputs.learning_rate}}', 'code': '/subscriptions/a4cdf085-0f39-444f-9cdc-ec6e91dfedf3/resourceGroups/a.anding-rg/providers/Microsoft.MachineLearningServices/workspaces/safira-ml/codes/27be9947-0e30-489c-b31e-15d7baf0367f/versions/1', 'environment_variables': {}, 'environment': '/subscriptions/a4cdf085-0f39-444f-9cdc-ec6e91dfedf3/resourceGroups/a.anding-rg/providers/Microsoft.MachineLearningServices/workspaces/safira-ml/environments/ml-use-case-1/versions/19', 'distribution': <azure.ai.ml.entities._job.distribution.PyTorchDistribution object at 0x7f82741ffe20>, 'resources': None, 'queue_settings': None, 'version': None, 'latest_version': None, 'schema': None, 'type': 'command', 'display_name': 'olive_ticket_d3jylgfzrb', 'is_deterministic': True, 'inputs': {'data_dir': {'type': 'uri_file', 'path': 'azureml://subscriptions/a4cdf085-0f39-444f-9cdc-ec6e91dfedf3/resourcegroups/a.anding-rg/workspaces/safira-ml/datastores/workspaceblobstore/paths/UI/2023-07-19_013839_UTC/output_fashion.csv', 'mode': 'ro_mount'}, 'epochs': {'type': 'string', 'default': '50'}, 'batch_size': {'type': 'string', 'default': '16'}, 'learning_rate': {'type': 'string', 'default': '0.002'}}, 'outputs': {'default': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.olive_ticket_d3jylgfzrb', 'mode': 'rw_mount'}}, 'yaml_str': None, 'other_parameter': {'status': 'Starting', 'parameters': {}}, 'additional_includes': [], 'CommandComponent__additional_includes_obj': None}), 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': {'Tracking': {'endpoint': 'azureml://eastus.api.azureml.ms/mlflow/v1.0/subscriptions/a4cdf085-0f39-444f-9cdc-ec6e91dfedf3/resourceGroups/a.anding-rg/providers/Microsoft.MachineLearningServices/workspaces/safira-ml?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/olive_ticket_d3jylgfzrb?wsid=/subscriptions/a4cdf085-0f39-444f-9cdc-ec6e91dfedf3/resourcegroups/a.anding-rg/workspaces/safira-ml&tid=3c243ae2-542b-4e73-9e4f-525c99011556', 'type': 'Studio'}}, 'status': 'Starting', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f82741fe770>}, 'instance_id': '0655e1fa-e761-4316-8553-f9b71df379f3', 'source': 'BUILDER', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': <azure.ai.ml.entities._job.distribution.PyTorchDistribution object at 0x7f82741ffe20>, 'environment_variables': {}, 'environment': 'ml-use-case-1:19', 'resources': {'instance_count': 1, 'shm_size': '2g'}, 'queue_settings': None, 'swept': False})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ml_client.jobs.create_or_update(job)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1689740966072
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# we will reuse the command_job created before. we call it as a function so that we can apply inputs\n",
        "# we do not apply the 'iris_csv' input again -- we will just use what was already defined earlier\n",
        "from azure.ai.ml.sweep import Choice, Uniform, MedianStoppingPolicy,LogUniform\n",
        "command_job_for_sweep = job(\n",
        "    learning_rate= LogUniform(2e-3, 5e-5),\n",
        "    batch_size=Choice(values=[2, 4, 8, 16]),\n",
        ")\n",
        "\n",
        "# apply the sweep parameter to obtain the sweep_job\n",
        "sweep_job = command_job_for_sweep.sweep(\n",
        "    compute=\"gpu\",  # Change the name to the gpu cluster of your workspace.\n",
        "    sampling_algorithm=\"random\",\n",
        "    primary_metric=\"validation_accuracy\",\n",
        "    goal=\"Maximize\",\n",
        "    max_total_trials=8,\n",
        "    max_concurrent_trials=4,\n",
        "   early_termination_policy = MedianStoppingPolicy(delay_evaluation = 10, evaluation_interval = 1),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1689740984259
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Your file exceeds 100 MB. If you experience low speeds, latency, or broken connections, we recommend using the AzCopyv10 tool for this file transfer.\n",
            "\n",
            "Example: azcopy copy '/mnt/batch/tasks/shared/LS_root/mounts/clusters/safira-cpu/code/Users/r.mandal/UC1_bio' 'https://safiraml9781015086.blob.core.windows.net/584b2160-4-3aae912c-0db5-5ff9-b522-1452554edc10/UC1_bio' \n",
            "\n",
            "See https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-v10 for more information.\n",
            "\u001b[32mUploading UC1_bio (141.2 MBs): 100%|██████████| 141197539/141197539 [00:01<00:00, 119764380.99it/s]\n",
            "\u001b[39m\n",
            "\n"
          ]
        },
        {
          "ename": "HttpResponseError",
          "evalue": "(UserError) Validation failed.\n Error : Expected 5E-05 to be greater than or equal to 0.002\n\nCode: UserError\nMessage: Validation failed.\n Error : Expected 5E-05 to be greater than or equal to 0.002\n\nAdditional Information:Type: ComponentName\nInfo: {\n    \"value\": \"managementfrontend\"\n}Type: Correlation\nInfo: {\n    \"value\": {\n        \"operation\": \"0f3025731cffb859f9fd59f14f35dd4b\",\n        \"request\": \"f354cf524a5db642\"\n    }\n}Type: Environment\nInfo: {\n    \"value\": \"eastus\"\n}Type: Location\nInfo: {\n    \"value\": \"eastus\"\n}Type: Time\nInfo: {\n    \"value\": \"2023-07-19T04:29:43.7938891+00:00\"\n}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# submit the sweep job\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m returned_sweep_job_cmd \u001b[38;5;241m=\u001b[39m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_or_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43msweep_job\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# get a URL for the status of the job\u001b[39;00m\n\u001b[1;32m      5\u001b[0m returned_sweep_job_cmd\u001b[38;5;241m.\u001b[39mstudio_url\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/tracing/decorator.py:76\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_telemetry/activity.py:337\u001b[0m, in \u001b[0;36mmonitor_with_telemetry_mixin.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m dimensions \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparameter_dimensions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(custom_dimensions \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, dimensions) \u001b[38;5;28;01mas\u001b[39;00m activityLogger:\n\u001b[0;32m--> 337\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parameter_dimensions:\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;66;03m# collect from return if no dimensions from parameter\u001b[39;00m\n\u001b[1;32m    340\u001b[0m         activityLogger\u001b[38;5;241m.\u001b[39mactivity_info\u001b[38;5;241m.\u001b[39mupdate(_collect_from_return_value(return_value))\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_job_operations.py:609\u001b[0m, in \u001b[0;36mJobOperations.create_or_update\u001b[0;34m(self, job, description, compute, tags, experiment_name, skip_validation, **kwargs)\u001b[0m\n\u001b[1;32m    607\u001b[0m     log_and_raise_error(ex)\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 609\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_job_operations.py:561\u001b[0m, in \u001b[0;36mJobOperations.create_or_update\u001b[0;34m(self, job, description, compute, tags, experiment_name, skip_validation, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (rest_job_resource\u001b[38;5;241m.\u001b[39mproperties\u001b[38;5;241m.\u001b[39mjob_type \u001b[38;5;241m==\u001b[39m RestJobType\u001b[38;5;241m.\u001b[39mPIPELINE) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(rest_job_resource\u001b[38;5;241m.\u001b[39mproperties, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(rest_job_resource\u001b[38;5;241m.\u001b[39mproperties\u001b[38;5;241m.\u001b[39midentity, UserIdentity))\n\u001b[1;32m    558\u001b[0m ):\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_headers_with_user_aml_token(kwargs)\n\u001b[0;32m--> 561\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_operation_2023_02_preview\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_or_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrest_job_resource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresource_group_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_operation_scope\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresource_group_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkspace_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_workspace_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrest_job_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_local_run(result):\n\u001b[1;32m    570\u001b[0m     ws_base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_all_operations\u001b[38;5;241m.\u001b[39mall_operations[\n\u001b[1;32m    571\u001b[0m         AzureMLResourceType\u001b[38;5;241m.\u001b[39mWORKSPACE\n\u001b[1;32m    572\u001b[0m     ]\u001b[38;5;241m.\u001b[39m_operation\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_base_url\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/tracing/decorator.py:76\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_restclient/v2023_04_01_preview/operations/_jobs_operations.py:760\u001b[0m, in \u001b[0;36mJobsOperations.create_or_update\u001b[0;34m(self, resource_group_name, workspace_name, id, body, **kwargs)\u001b[0m\n\u001b[1;32m    758\u001b[0m     map_error(status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m=\u001b[39mresponse, error_map\u001b[38;5;241m=\u001b[39merror_map)\n\u001b[1;32m    759\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize\u001b[38;5;241m.\u001b[39mfailsafe_deserialize(_models\u001b[38;5;241m.\u001b[39mErrorResponse, pipeline_response)\n\u001b[0;32m--> 760\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response\u001b[38;5;241m=\u001b[39mresponse, model\u001b[38;5;241m=\u001b[39merror, error_format\u001b[38;5;241m=\u001b[39mARMErrorFormat)\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    763\u001b[0m     deserialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJobBase\u001b[39m\u001b[38;5;124m'\u001b[39m, pipeline_response)\n",
            "\u001b[0;31mHttpResponseError\u001b[0m: (UserError) Validation failed.\n Error : Expected 5E-05 to be greater than or equal to 0.002\n\nCode: UserError\nMessage: Validation failed.\n Error : Expected 5E-05 to be greater than or equal to 0.002\n\nAdditional Information:Type: ComponentName\nInfo: {\n    \"value\": \"managementfrontend\"\n}Type: Correlation\nInfo: {\n    \"value\": {\n        \"operation\": \"0f3025731cffb859f9fd59f14f35dd4b\",\n        \"request\": \"f354cf524a5db642\"\n    }\n}Type: Environment\nInfo: {\n    \"value\": \"eastus\"\n}Type: Location\nInfo: {\n    \"value\": \"eastus\"\n}Type: Time\nInfo: {\n    \"value\": \"2023-07-19T04:29:43.7938891+00:00\"\n}"
          ]
        }
      ],
      "source": [
        "# submit the sweep job\n",
        "returned_sweep_job = ml_client.jobs.create_or_update(sweep_job)\n",
        "\n",
        "# stream the output and wait until the job is finished\n",
        "ml_client.jobs.stream(returned_sweep_job.name)\n",
        "\n",
        "# refresh the latest status of the job after streaming\n",
        "returned_sweep_job = ml_client.jobs.get(name=returned_sweep_job.name)\n",
        "\n",
        "# get a URL for the status of the job\n",
        "returned_sweep_job.studio_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1689893275904
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Handle to the workspace\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "# Authentication package\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "\n",
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    # Check if given credential can get token successfully.\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "    credential = InteractiveBrowserCredential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1689647509760
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=\"\",\n",
        "    resource_group_name=\"\",\n",
        "    workspace_name=\"\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1689647510462
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "# Creating a unique name for the endpoint\n",
        "online_endpoint_name = \"uc1-\" + str(uuid.uuid4())[:8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1689647608435
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Endpint uc1-37e94cda provisioning state: Succeeded\n"
          ]
        }
      ],
      "source": [
        "from azure.ai.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    ManagedOnlineDeployment,\n",
        "    Model,\n",
        "    Environment,\n",
        ")\n",
        "\n",
        "# create an online endpoint\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=online_endpoint_name,\n",
        "    description=\"this is an online endpoint for uc1\",\n",
        "    auth_mode=\"key\",\n",
        "    tags={\n",
        "        \"training_dataset\": \"product dataste\",\n",
        "        \"model_type\": \"OpenTag\",\n",
        "    },\n",
        ")\n",
        "\n",
        "endpoint_result = ml_client.begin_create_or_update(endpoint).result()\n",
        "\n",
        "print(\n",
        "    f\"Endpint {endpoint_result.name} provisioning state: {endpoint_result.provisioning_state}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1689647608912
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Endpint \"uc1-37e94cda\" with provisioning state \"Succeeded\" is retrieved\n"
          ]
        }
      ],
      "source": [
        "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
        "\n",
        "print(\n",
        "    f'Endpint \"{endpoint.name}\" with provisioning state \"{endpoint.provisioning_state}\" is retrieved'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1689647609568
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Let's pick the latest version of the model\n",
        "latest_model_version = max(\n",
        "    [int(m.version) for m in ml_client.models.list(name=\"uc1_token\")]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1689896323250
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "env = Environment(\n",
        "    conda_file=os.path.join(\"./dependencies\", \"conda.yaml\"),\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1689896324119
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'latest_model_version' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01midentity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultAzureCredential\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# picking the model to deploy. Here we use the latest version of our registered model\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m ml_client\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mget(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muc1_token\u001b[39m\u001b[38;5;124m\"\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[43mlatest_model_version\u001b[49m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# create an online deployment.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m red_deployment_uc1 \u001b[38;5;241m=\u001b[39m ManagedOnlineDeployment(\n\u001b[1;32m     17\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muc1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     endpoint_name\u001b[38;5;241m=\u001b[39monline_endpoint_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     instance_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     27\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'latest_model_version' is not defined"
          ]
        }
      ],
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    ManagedOnlineDeployment,\n",
        "    Model,\n",
        "    CodeConfiguration,\n",
        "    Environment,\n",
        ")\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# picking the model to deploy. Here we use the latest version of our registered model\n",
        "model = ml_client.models.get(name=\"uc1_token\", version=latest_model_version)\n",
        "\n",
        "\n",
        "# create an online deployment.\n",
        "red_deployment_uc1 = ManagedOnlineDeployment(\n",
        "    name=\"uc1\",\n",
        "    endpoint_name=online_endpoint_name,\n",
        "    model=model,\n",
        "    code_configuration=CodeConfiguration(\n",
        "        code=\"./\",\n",
        "        scoring_script=\"score.py\"\n",
        "    ),\n",
        "    environment=env,\n",
        "    instance_type=\"Standard_E2s_v3\",\n",
        "    instance_count=1,\n",
        ")\n",
        "\n",
        "red_deployment_results = ml_client.online_deployments.begin_create_or_update(\n",
        "    red_deployment_uc1\n",
        ").result()\n",
        "\n",
        "print(\n",
        "    f\"Deployment {red_deployment_results.name} provisioning state: {red_deployment_results.provisioning_state}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1689842525784
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'{\"status\": \"success\", \"result\": [{\"general\": [{\"types\": [\"Dog supplies\"], \"all_attributes\": [{\"Title\": \"string\", \"Description\": \"string\", \"Price\": \"decimal\", \"Category\": \"string\", \"Type\": \"string\", \"Image\": \"file\"}], \"common_attributes\": [{\"Title\": \"string\", \"Description\": \"string\", \"Price\": \"file\", \"Category\": \"string\"}]}]}], \"data\": [{\"Title\": [{\"input\": \"Gaok Men\\'s Retro Cotton Multi-Pocket Camo Cargo Shorts\", \"status\": \"mapped\", \"tags\": [{\"Gaok\": [{\"attribute\": \"BRAND\"}]}, {\"Camo Cargo Shorts\": [{\"attribute\": \"PATTERN\"}]}]}], \"Description\": [{\"input\": \"Random Description\", \"status\": \"unmapped\"}], \"Price\": [{\"input\": \"13\", \"status\": \"unmapped\"}], \"Category\": [{\"input\": \"Jeany Jeans\", \"status\": \"mapped\", \"tags\": {\"Jeany Jeans\": [{\"attribute\": \"category\", \"probability\": \"100%\"}]}}], \"Image\": [{\"input\": \"https://www.ssxsds.de/test.jpg\", \"status\": \"unmapped\"}]}, {\"Title\": [{\"input\": \"Honchosfx Mens Retro Creedence Clearwater Revival 1971 T-Shirt Brown\", \"status\": \"mapped\", \"tags\": [{\"Honchosfx\": [{\"attribute\": \"BRAND\"}]}]}], \"Description\": [{\"input\": \"ssdsdsdssdssd\", \"status\": \"unmapped\"}], \"Price\": [{\"input\": \"15\", \"status\": \"unmapped\"}], \"Category\": [{\"input\": \"random\", \"status\": \"mapped\", \"tags\": {\"random\": [{\"attribute\": \"category\", \"probability\": \"100%\"}]}}], \"Image\": [{\"input\": \"https://www.ssxsds.de/test.jpg\", \"status\": \"unmapped\"}]}]}'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ml_client.online_endpoints.invoke(\n",
        "    endpoint_name=\"uc1-37e94cda\",\n",
        "    deployment_name=\"uc1\",\n",
        "    request_file=os.path.join(\"./dependencies\", \"sam.json\"),\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
